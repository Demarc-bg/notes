###Intro

#####Machine Learning compose of three parts

1. Data: Different applications has different Data
2. Model(function): Define specific model (like Recommendation system)
3. Loss(prediction): Model evaluation(Hinge loss Logistic loss Softmax loss)

Therefore, Machine Learning ≈ Looking for a Function.

#####Framework: 

- A set of function
- Model: f1,f2...

#####Three Steps:

- **define** a set of function
- **goodness** of function
- pick the **best** function

#####Data：

$D = {(xi, yi)}n,i=1$

x is input, and we usually present it as column vector, 

y isoutput (for example: name of a person), n is number.

##### loss function:

$yˆ = f ( x )$

Absolute value loss

Least squares loss（最小二乘法）

$L_D(w) = \sum_{i=1}^N􏰁 l(y_{ˆi}, y_i)$

目标是：让$L_D(w)$ 尽可能地小

------

###Linear regression

#####**Closed-form solution** to linear regression:

1. Loss function:

-  $L_D(w) =1/2(y-Xw)^T(y-Xw)$
- $L_D(w) = 1/2(y^Ty-2w^TX^Ty+w^TX^TXw)$

2. parameter w：

- $\frac{\partial L_D(w)}{\partial w}=-X^Ty+X^TXw$

- $w=(X^TX)^{-1}X^Ty$


3. Set learning rate: 

- too large ->diverge

- too small->too slow to converge

#####**Adaptive learning rate**自适应学习率

- Set larger learning rate at the begining
- Use relatively smaller learning rate in the later epochs
- Decrease the learning rate: $η^{t+1} = \frac{η^{t}}{t+1}$ 

------

### Linear classification

In 3D the discriminant is a plane, and in mD it is a hyperplane

------

###SVM

**Margin**: The region bounded by these **two hyperplanes** is called the ”margin”

Select two parallel hyperplanes that **separate** the two classes of data and let the distance between them as large as possible

Tactics: **Maximum margin**

*In general there is a **trade off** between the margin and the number of mistakes on the training data*

------

### Multi-class classification

1. Three general strategies

   - Transformation to binary classification
     - One-vs.-rest: 
       - the number of classifier is K 
       - 每次进行分类，会有n次one vs rest的过程，取置信度最高的那类
     - One-vs.-one: 
       - the number of classifier is $\frac{1}{2}k(k-1)$
       - 采用投票vote的方法，进行$\frac{1}{2}k(k-1)$ 次的比较、加法之后，取投票数最高的那类
     - Decision Directed Acyclic Graph, DDAG:
       - when classification, only use k-1 classifiers
   - Extension from binary classification
     - Neural networks
     - Decision trees
     - K-nearest neighbours
     - Softmax function

   - Hierarchical classification
     - label tree



------

### Ensemble  

#### Decision Tree

Entropy 信息墒:

$info(D)=-\sum_{i=1}p_ilog_2{p_i}$

$Gain(D)=Entropy(D)-\sum Entropy(D)*D_v/D$

以Gain最大为选择度量的属性，直到info为0



#### Adaboost

实例讲解：https://blog.csdn.net/guyuealian/article/details/70995333

实例详细步骤：

1. 每个弱分类器都会有一个 $\alpha$ 系数，构成最终的强分类器中该弱分类器的权重，而 $\alpha$ 又依赖基于上一代权值$D(t-1)$ 计算出的误差率e 

   alpha 公式为： $\alpha =\frac{1}{2}ln(\frac{1-e}{e})$   *Make the base learner with **lower $e$** more important*

   也就是说，每一代重新选择弱分类器的时候，是根据基于权值分布$D(t-1)$计算得出的误差率，来选择弱分类器，（选误差率最低的那个弱分类器）然后在准备开始的迭代中： 

   *每一次从弱分类器集合中重新选择弱分类器*

   - 计算**当前弱分类器权重$\alpha$** 
   - 根据之前算出的误差率e **更新当前迭代次权值分布**$D(t)$

2. 然后下一次迭代中，都会选择上一代t更新之后的权值$D(t)$  

##### **Note**：

Adaboost can deal with **nonlinear problem**

Regularization:

- Trade off between fitting the training set well and keeping the model relatively simple



------

###Validation

Underfitting:Model cannot capture the underlying trend of the data

- increase the capacity(the complexity of the model)

Overfitting: 

- lower down complexity
- increase the training set
- regularization(正则化)

Split data set into 

• Learning set 

• Validation set 

• Test set 

------

### PCA

主成分分析（其实就是数据降维）：寻找数据在某个向量方向（可为组合的多维向量空间）上最大特征值方差，而另一个向量方向方差十分小，将数据全部映射到组合的多维向量空间中。

------

### Cluster

####k-means

- 初始阶段，设置k个质心并随机抽取数据中的点赋值给它
- 收敛阶段（loop）
  - 对于每个点，将其指向离它最近的质心（需要**计算最小的欧式距离**）
  - 对于每个质心，根据指向它的数据点，**计算数据点的平均中心点**，更新质心 

------

### Image Procoessing

Image store in matrices

####Two parts of the image:

- intensity强度：
  - 含有更多的信息

- chrominance色度

#### Interdisciplinary Fields

图像处理和计算机视觉是交叉学科领域

- 数学模型（CS，EE，math）
- 视觉研究（Biology）

####具体内容

- 图像去噪
- 图像去模糊
- 图像增强
- 图像修补
- 视频修复
- 纹理合成

#### 工业应用

- 自动驾驶辅助
- 数码图片处理
- 体育运动分析
- 图像数据库（人脸识别）





